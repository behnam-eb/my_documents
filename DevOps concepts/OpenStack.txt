Swift
https://insights.ubuntu.com/2015/05/18/what-are-the-different-types-of-storage-block-object-and-file/
https://en.wikipedia.org/wiki/Object_storage

on all servers : vim.tiny /etc/hosts
# controller
10.0.0.11       controller
# compute1
10.0.0.31       compute1
# block1
10.0.0.41       block1
# object1
10.0.0.51       object1
# object2
10.0.0.52       object2
+ 127.0.0.1 if exists

For simplicity, this guide installs and configures the proxy service on the controller node. 
The proxy service relies on an authentication and authorization mechanism such as the Identity service. 
However, unlike other services, it also offers an internal mechanism that allows it to operate without any other OpenStack services.
Before you configure the Object Storage service, you must create service credentials and an API endpoint.
----------------------------------------------------
On all nodes:
 apt-get install software-properties-common
 add-apt-repository cloud-archive:ocata
 apt-get update
 apt-get upgrade
 apt-get install python-openstackclient
----------------------------------------------------
----------------------------------------------------
Controller node:
 apt-get install mariadb-server python-pymysql
 vim.tiny /etc/mysql/mariadb.conf.d/99-openstack.cnf
	[mysqld]
	bind-address = 10.0.0.11(ip of controller)
	default-storage-engine = innodb
	innodb_file_per_table = on
	max_connections = 4096
	collation-server = utf8_general_ci
	character-set-server = utf8
 service mysql restart
 mysql_secure_installation
 
 apt-get install rabbitmq-server  => for message queue
 rabbitmqctl add_user openstack [RABBIT_PASS] => [RABBIT_PASS]=salamsalam
 rabbitmqctl set_permissions openstack ".*" ".*" ".*"
 
The Identity service authentication mechanism for services uses Memcached to cache tokens.
 apt-get install memcached python-memcache
 vim.tiny /etc/memcached.conf
	-l 10.0.0.11(ip of controller) => This is to enable access by other nodes via the management network
 service memcached restart
------------ 
install and configure the OpenStack Identity service, code-named keystone, on the controller node.
For scalability purposes, this configuration deploys Fernet tokens and the Apache HTTP server to handle requests.
 mysql
	CREATE DATABASE keystone;
	GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'KEYSTONE_DBPASS'; => KEYSTONE_DBPASS=salamsalam
	GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'KEYSTONE_DBPASS'; => KEYSTONE_DBPASS=salamsalam
 apt-get install keystone
 vim.tiny /etc/keystone/keystone.conf
	[database]
	# ...
	connection = mysql+pymysql://keystone:salamsalam@192.168.116.136:3306/keyston=> KEYSTONE_DBPASS=salamsalam
	[token]
	# ...
	provider = fernet
 Populate the Identity service database:
	su -s /bin/sh -c "keystone-manage db_sync" keystone
 keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
 keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
 
 keystone-manage bootstrap --bootstrap-password  salamsalam --bootstrap-admin-url http://swift-proxy:35357/v3/ --bootstrap-internal-url http://swift-proxy:5000/v3/ --bootstrap-public-url http://swift-proxy:5000/v3/ --bootstrap-region-id RegionOne
 
 vim.tiny /etc/apache2/apache2.conf
	ServerName controller
 service apache2 restart
 rm -f /var/lib/keystone/keystone.db
 
 export OS_USERNAME=admin
 export OS_PASSWORD=salamsalam
 export OS_PROJECT_NAME=admin
 export OS_USER_DOMAIN_NAME=Default
 export OS_PROJECT_DOMAIN_NAME=Default
 export OS_AUTH_URL=http://swift-proxy:35357/v3
 export OS_IDENTITY_API_VERSION=3
 
The Identity service provides authentication services for each OpenStack service.
The authentication service uses a combination of domains, projects, users, and roles.
This guide uses a service project that contains a unique user for each service that you add to your environment. Create the service project:
 openstack project create --domain default --description "Service Project" service
 
Regular (non-admin) tasks should use an unprivileged project and user. As an example, this guide creates the demo project and user.
 openstack project create --domain default --description "Demo Project" demo
 openstack user create --domain default --password-prompt demo => password=qazwsx
 openstack role create user
 openstack role add --project demo --user demo user
You can repeat this procedure to create additional projects and users.
For security reasons, disable the temporary authentication token mechanism:
	vim.tiny /etc/keystone/keystone-paste.ini
		remove admin_token_auth from the [pipeline:public_api], [pipeline:admin_api], and [pipeline:api_v3] sections
	unset OS_AUTH_URL OS_PASSWORD
	
Verify operation:
 openstack --os-auth-url http://swift-proxy:35357/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name admin --os-username admin token issue
	password=salamsalam <= This command uses the password for the admin user.
 openstack --os-auth-url http://swift-proxy:5000/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name demo --os-username demo token issue
	password=qazwsx <= This command uses the password for the demo user and API port 5000 which only allows regular (non-admin) access to the Identity service API.

The previous section used a combination of environment variables and command options to interact with the Identity service via the openstack client. To increase efficiency of client operations, OpenStack supports simple client environment scripts also known as OpenRC files.
These scripts typically contain common options for all clients, but also support unique options. 
Create client environment scripts for the admin and demo projects and users. Future portions of this guide reference these scripts to load appropriate credentials for client operations.
Create and edit the admin-openrc file and add the following content:
vim.tiny admin-openrc
	export OS_PROJECT_DOMAIN_NAME=Default
	export OS_USER_DOMAIN_NAME=Default
	export OS_PROJECT_NAME=admin
	export OS_USERNAME=admin
	export OS_PASSWORD=ADMIN_PASS  => salamsalam
	export OS_AUTH_URL=http://swift-proxy:35357/v3
	export OS_IDENTITY_API_VERSION=3
	export OS_IMAGE_API_VERSION=2
vim.tiny demo-openrc
	export OS_PROJECT_DOMAIN_NAME=Default
	export OS_USER_DOMAIN_NAME=Default
	export OS_PROJECT_NAME=demo
	export OS_USERNAME=demo
	export OS_PASSWORD=DEMO_PASS => qazwsx
	export OS_AUTH_URL=http://swift-proxy:5000/v3
	export OS_IDENTITY_API_VERSION=3
	export OS_IMAGE_API_VERSION=2
------------
proxy installation:
Source the admin credentials to gain access to admin-only CLI commands:
 . admin-openrc
To create the Identity service credentials, complete these steps:
 Create the swift user:
	openstack user create --domain default --password-prompt swift
		password=Aa12345
 Add the admin role to the swift user:
	openstack role add --project service --user swift admin
 Create the swift service entity:
	openstack service create --name swift --description "OpenStack Object Storage" object-store
 Create the Object Storage service API endpoints:
	openstack endpoint create --region RegionOne object-store public http://swift-proxy:8080/v1/AUTH_%\(tenant_id\)s
	openstack endpoint create --region RegionOne object-store internal http://swift-proxy:8080/v1/AUTH_%\(tenant_id\)s
	openstack endpoint create --region RegionOne object-store admin http://swift-proxy:8080/v1
Install the packages:
 apt-get install swift swift-proxy python-swiftclient python-keystoneclient python-keystonemiddleware memcached
Obtain the proxy service configuration file from the Object Storage source repository:
 curl -o /etc/swift/proxy-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/proxy-server.conf-sample?h=stable/pcata
 
 mkdir -p /etc/swift
vim.tiny /etc/swift/proxy-server.conf
	[DEFAULT]
	...
	bind_port = 8080
	user = swift
	swift_dir = /etc/swift
	*In the [pipeline:main] section, remove the tempurl and tempauth modules and add the authtoken and keystoneauth modules:(Do not change the order of the modules.)
	pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server
	[app:proxy-server]
	use = egg:swift#proxy
	...
	account_autocreate = True
	[filter:keystoneauth]
	use = egg:swift#keystoneauth
	...
	operator_roles = admin,user
	[filter:authtoken]
	paste.filter_factory = keystonemiddleware.auth_token:filter_factory
	...
	auth_uri = http://swift-proxy:5000
	auth_url = http://swift-proxy:35357 (keystonehost)
	memcached_servers = swift-proxy:11211
	auth_type = password
	project_domain_name = default
	user_domain_name = default
	project_name = service
	username = swift
	password = SWIFT_PASS    => Aa12345
	delay_auth_decision = True
	[filter:cache]
	use = egg:swift#memcache
	...
	memcache_servers = swift-proxy:11211
	
----------------------------------------------------
----------------------------------------------------	
storage nodes:
first,add hard to your storage servers !
##Install the supporting utility packages:
 apt-get install xfsprogs rsync
 mkfs.xfs /dev/sdb
 mkfs.xfs /dev/sdc
 ...
 mkdir -p /srv/node/sdb
 mkdir -p /srv/node/sdc
vim.tiny /etc/fstab
	/dev/sdb /srv/node/sdb xfs noatime,nodiratime,nobarrier,logbufs=8 0 2
	/dev/sdc /srv/node/sdc xfs noatime,nodiratime,nobarrier,logbufs=8 0 2
mount -a
vim.tiny /etc/rsyncd.conf
	uid = swift
	gid = swift
	log file = /var/log/rsyncd.log
	pid file = /var/run/rsyncd.pid
	address = MANAGEMENT_INTERFACE_IP_ADDRESS =>IP address of the management network on the storage node
	[account]
	max connections = 2
	path = /srv/node/
	read only = False
	lock file = /var/lock/account.lock
	[container]
	max connections = 2
	path = /srv/node/
	read only = False
	lock file = /var/lock/container.lock
	[object]
	max connections = 2
	path = /srv/node/
	read only = False
	lock file = /var/lock/object.lock
vim.tiny /etc/default/rsync
	RSYNC_ENABLE=true
service rsync start
##Install and configure components:
apt-get install swift swift-account swift-container swift-object
Obtain the accounting, container, and object service configuration files from the Object Storage source repository:
 curl -o /etc/swift/account-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/account-server.conf-sample?h=stable/ocata
 curl -o /etc/swift/container-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/container-server.conf-sample?h=stable/ocata
 curl -o /etc/swift/object-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/object-server.conf-sample?h=stable/ocata
 
 vim.tiny /etc/swift/account-server.conf
	 [DEFAULT]
	...
	bind_ip = MANAGEMENT_INTERFACE_IP_ADDRESS
	bind_port = 6202
	user = swift
	swift_dir = /etc/swift
	devices = /srv/node
	mount_check = True
	[pipeline:main]
	pipeline = healthcheck recon account-server
	[filter:recon]
	use = egg:swift#recon
	...
	recon_cache_path = /var/cache/swift
	
vim.tiny /etc/swift/container-server.conf
	[DEFAULT]
	...
	bind_ip = MANAGEMENT_INTERFACE_IP_ADDRESS
	bind_port = 6201
	user = swift
	swift_dir = /etc/swift
	devices = /srv/node
	mount_check = True
	[pipeline:main]
	pipeline = healthcheck recon container-server
	[filter:recon]
	use = egg:swift#recon
	...
	recon_cache_path = /var/cache/swift
	
vim.tiny /etc/swift/object-server.conf
	[DEFAULT]
	...
	bind_ip = MANAGEMENT_INTERFACE_IP_ADDRESS
	bind_port = 6200
	user = swift
	swift_dir = /etc/swift
	devices = /srv/node
	mount_check = True
	[pipeline:main]
	pipeline = healthcheck recon object-server
	[filter:recon]
	use = egg:swift#recon
	...
	recon_cache_path = /var/cache/swift
	recon_lock_path = /var/lock
	
chown -R swift:swift /srv/node
mkdir -p /var/cache/swift
chown -R root:swift /var/cache/swift
chmod -R 775 /var/cache/swift
----------------------------------------------------
----------------------------------------------------
Before starting the Object Storage services, you must create the initial account, container, and object rings. 
The ring builder creates configuration files that each node uses to determine and deploy the storage architecture.
For simplicity, this guide uses one region and two zones with 2^10 (1024) maximum partitions, 3 replicas of each object, and 1 hour minimum time between moving a partition more than once.
For Object Storage, a partition indicates a directory on a storage device rather than a conventional partition table. For more information
The Proxy Server is responsible for tying together the rest of the Swift architecture. For each request, it will look up the location of the account, container, or object in the ring (see below) and route the request accordingly. 
For Erasure Code type policies, the Proxy Server is also responsible for encoding and decoding object data. See Erasure Code Support for complete information on Erasure Code support. The public API is also exposed through the Proxy Server.
A large number of failures are also handled in the Proxy Server. For example, if a server is unavailable for an object PUT, it will ask the ring for a handoff server and route there instead.
When objects are streamed to or from an object server, they are streamed directly through the proxy server to or from the user – the proxy server does not spool them.
A ring represents a mapping between the names of entities stored on disk and their physical location. There are separate rings for accounts, containers, and one object ring per storage policy. 
When other components need to perform any operation on an object, container, or account, they need to interact with the appropriate ring to determine its location in the cluster.
The Ring maintains this mapping using zones, devices, partitions, and replicas. Each partition in the ring is replicated, by default, 3 times across the cluster, and the locations for a partition are stored in the mapping maintained by the ring. The ring is also responsible for determining which devices are used for handoff in failure scenarios.


##Perform these steps on the controller node.
The account server uses the account ring to maintain lists of containers.
 cd /etc/swift
 swift-ring-builder account.builder create 10 3 1
Add each storage node to the ring:
 swift-ring-builder account.builder add --region 1 --zone 1 --ip 192.168.116.137 --port 6202 --device sdb --weight 100
 swift-ring-builder account.builder add --region 1 --zone 1 --ip 192.168.116.137 --port 6202 --device sdc --weight 100
 swift-ring-builder account.builder add --region 1 --zone 1 --ip 192.168.116.138 --port 6202 --device sdb --weight 100
 swift-ring-builder account.builder add --region 1 --zone 1 --ip 192.168.116.138 --port 6202 --device sdc --weight 100
 ...
Verify the ring contents:
 swift-ring-builder account.builder
 یادت باشه پارتیشنای یه سرور را حذف کرده بودی درستش کونی بعد برو سراغ بقیش